## :dollar: ANALYSIS AND ML PREDICTION ON THE ADULT CENSUS DATASET

### :open_book: OVERVIEW
Date: December 2024\
Author(s): Ashneet Rathore

 The [**Census Income dataset**](https://archive.ics.uci.edu/dataset/20/census+income), otherwise known as the Adult dataset, contains 14 features, which are a mix of categorical and integer types. The target variable has two labels: `<=50K` and `>50K`. This dataset can be used for **binary classification** tasks, where the goal is to predict whether or not an individual’s annual income exceeds $50,000 based on 1994 census data. The following sections present an exploratory analysis of the dataset and the accuracy performance of three different boosting algorithms – **AdaBoost**, **Gradient Boosting**, and **XGBoost** – across different parameter settings.

> [!NOTE]
> All figures referenced in this report can be found in the corresponding Jupyter Notebook (`project.ipynb`) in the repository.

### :bar_chart: EXPLORATORY ANALYSIS
During the analysis phase, the relationships between various features and the likelihood of earning more than $50,000 were examined to identify characteristics associated with higher income.

Figure 1 displays the percentage of people earning more than $50,000 categorized by sex. The bar chart reveals that while 30% of males earned more than $50,000 in 1994, only about 10% of all females did. This disparity reflects broader trends in the 1990s: although women were increasingly entering the workforce, they were still underrepresented in high-paying roles and often earned less than their male counterparts for comparable work.

Figure 2 displays the percentage of people earning more than $50,000 categorized by age. The plot shows that middle-aged adults, ranging from 40 to 60 years old, had the highest percentage of income greater than $50,000, with the peak occurring at age 50. This upside-down U shape aligns with typical career trends: while young adults are either in school or just beginning their careers, and older adults people are reducing their workload and preparing for retirement, middle-aged adults are typically in the prime of their established careers, more likely at higher-paying positions in the industries they work in.

Figure 3 shows the percentage of people earning more than $50,000 categorized by education level. Individuals with bachelor's and master's degrees have noticeably higher proportions of high earners compared to those with an associate degree, which displays the role that a four-year or higher degree can play in accessing better-paying jobs. The gap is even larger for those with doctorate or professional degrees, which aligns with the common trend that advanced degrees in specialized fields, such as law or medicine, are linked to higher incomes.

Figure 4 shows the frequency of adults in each occupation, separated by income category (`<=50K` and `>50K`). Each occupation has two bars representing the number of individuals earning 50K or less and those earning more than 50K. This visualization highlights which occupations tend to have more high-income earners and which are dominated by lower-income earners, providing a clear view of how income is distributed across different types of work. 

According to the chart, executive-managerial and professional-speciality positions have the highest proportion of individuals earning above $50,000 among all occupations. Roles involving technical manual labor and administrative tasks, such as administrative-clerical, craft-repair, and sales occupations, have a large gap between the two bars, with the majority earning $50,000 or below and only a small fraction earning above $50,000. Farming-fishing, service, and tech support roles show an even wider gap between the two bars, with relatively few high earners. These income patterns do not necessarily correspond to the societal importance of the occupations. For instance, farming plays a critical role in society, yet it shows a relatively low proportion of high-income earners, illustrating that essential work is not always matched with higher pay.

### :trophy: EVALUATION OF BOOSTING ALGORITHMS
Both AdaBoost and Gradient Boosting are sequential ensemble learning methods that combine multiple weak learners, such as decision trees, to create a stronger model that improves performance. AdaBoost prioritizes misclassified data by increasing its weight at each iteration, so subsequent learners pay more attention to difficult cases. Gradient Boosting, on the other hand, minimizes residual error by fitting new models to correct the mistakes of previous ones using gradient descent. XG Boosting is an optimized implementation of Gradient Boosting that adds regularization to prevent overfitting and support parallel processing. 

This section of the report compares the accuracies of the three algorithms by varying two parameters: the learning rate and the number of estimators, or weak learners. The evaluation uses a 70/30 train–test split, with 70% of the data used for training and 30% reserved for testing

Figure 5 depicts the accuracy of the three algorithms with a varying number of estimators, ranging from 25 to 425, increasing in increments of 25. For reference, a constant learning rate of 0.1 was used. AdaBoost has a huge jump in accuracy from 25 to 50 estimators, and after this point, the accuracy continues to increase, but at a much slower rate beyond 100 estimators. Gradient Boosting shows a more proportional, steady increase, peaking at 400 estimators, after which point it begins overfitting. However, beyond 175 estimators, there is no significant increase in accuracy (NOTE: "Significant increase" refers to the change observed at each 25-estimator interval, not the overall improvement in accuracy). XG Boosting shows a similar trend to Gradient Boosting, but it reaches its peak accuracy much earlier at 150 estimators. Gradient Boosting significantly outperforms AdaBoost in terms of accuracy, while slightly lagging behind XG Boosting. It is logical that XG Boosting performs better than Gradient Boosting because it’s an optimized version of it.

A larger range of estimators was included in the graph to highlight that for both AdaBoost and Gradient Boosting, even though peak accuracy occurs at higher estimator counts, the improvements beyond a certain point are negligible. Thus, it might not always be the smartest decision to choose a number of estimators that provides the highest accuracy. Using significantly more estimators for minimal accuracy gains leads to unnecessary computational cost, making it an inefficient choice.

Figure 6 shows the accuracy of the three algorithms with varying learning rates, ranging from 0.1 to 1, increasing in increments of 0.1. Although a constant number of estimators was used for this part, the specific value varied by algorithm based on the earlier findings - 100 for AdaBoost, 175 for Gradient Boosting, and 150 for XGBoost. AdaBoost steadily increases its performance, reaching its peak at a learning rate of 1.0, while Gradient Boosting reaches its peak at a learning rate of 0.2, after which it begins to overfit the data. XG Boosting reaches its peak at a learning rate of 0.1. Gradient Boosting again significantly outperforms AdaBoost, with its peak accuracy being very close to that of XG Boosting.

Gradient Boosting outperforms AdaBoost on this particular dataset. Not only does it achieve higher accuracy, but it does so with fewer estimators, resulting in lower overall computational cost. Additionally, the fact that Gradient Boosting reaches its peak accuracy at a lower learning rate is beneficial because smaller updates allows the model to learn the underlying structure of the data more reliably and makes it less prone to overfitting. XG Boosting was included to illustrate its performance relative to Gradient Boosting. It achieves a slightly higher accuracy with a lower computational cost. The superior performance of Gradient Boosting over AdaBoost can be attributed to the fact that the latter generally works better on smaller datasets, and because this dataset was large, it’s better to use Gradient Boosting or XG Boosting.